{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [**1. Multinomial Naive Bayes Classifier**](#1.-Multinomial-Naive-Bayes-Classifier)\n",
    "   - [**1.1 Preprocessing functions**](#1.1-Preprocessing-functions)\n",
    "   - [**1.2 Fit Function**](#1.2-Fit-Function)\n",
    "   - [**1.3 Prediction Functions**](#1.3-Prediction-Functions)\n",
    "- [**2. Preprocessing Data**](#2.-Preprocessing-Data)\n",
    "- [**3. Training Data**](#3.-Training-Data)\n",
    "- [**4. Testing Data**](#4.-Testing-Data)\n",
    "- [**5. Cross Check With Sklearn Classifier**](#5.-Cross-Check-With-Sklearn-Classifier)\n",
    "- [**6. Accuracy Comparison Between Self Implemented And Sklearn Classifier**](#6.-Accuracy-Comparison-Between-Self-Implemented-And-Sklearn-Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_dir(path):\n",
    "    \"\"\"This creates a dictionary which separates the directory files into \n",
    "    two dictionaries, namely, train & test directory\"\"\"\n",
    "    \n",
    "    class_directory = [file for file in listdir(path)] # Load directory folders\n",
    "    data_directory = {}\n",
    "    for dir_ in class_directory:\n",
    "        data_directory[dir_] = [file for file in listdir(path + '/' + dir_)] # Load directory files\n",
    "    data_directory_split_train = {}\n",
    "    data_directory_split_test = {}\n",
    "    for dir_ in data_directory: # Split into train & test dictionary\n",
    "        train_dr,test_dr = model_selection.train_test_split(data_directory[dir_], shuffle=False)\n",
    "        data_directory_split_train[dir_] = train_dr\n",
    "        data_directory_split_test[dir_] = test_dr\n",
    "    return data_directory_split_train,data_directory_split_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary(train_dir, path):\n",
    "    \"\"\"This function generates the vocabulary list from all the files. This function limits to the top\n",
    "    2000 words\"\"\"\n",
    "    \n",
    "    vocabulary_dict = {}\n",
    "    stop_words = stopwords.words('english') # Stopwords to exclude\n",
    "    block_words = ['newsgroups:', 'xref', 'path', 'from:', 'subject:', 'sender', 'organisation', 'apr','gmt', \n",
    "               'last','better','never','every','even','two','good','used','first','need','going','must',\n",
    "               'really','might','well','without','made','give','look','try','far','less','seem','new','make',\n",
    "               'many','way','since','using','take','help','thanks','send','free','may','see','much','want','find',\n",
    "               'would','one','like','get','use','also','could','say','us','go','please','said','set','got','sure',\n",
    "               'come','lot','seems','able','anything','put', '--', '|>', '>>', '93', 'xref', 'cantaloupe.srv.cs.cmu.edu',\n",
    "               '20', '16', \"max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'\", '21', '19', '10', '17', '24', \n",
    "               'reply-to:', 'thu', 'nntp-posting-host:', 're:','25''18'\"i'd\"'>i''22''fri,''23''>the','references:','xref:',\n",
    "               'sender:','writes:','1993','organization:','message-id:','lines:','i\\'m','distribution:','i\\'ve','can\\'t',\n",
    "               '>in','>the','that\\'s','>i','it','i\\'d','...','>|>','---','keywords:','followup-to:','//']\n",
    "    # Additional block words to be excluded\n",
    "    for dir_ in train_dir: # Adding words to the dictionary\n",
    "        for i in range(len(train_dir[dir_])):\n",
    "            current_path = path + '/' + dir_ + '/' + train_dir[dir_][i]\n",
    "            data_file = open(current_path,'r').read()\n",
    "            for word in data_file.split():\n",
    "                if len(word)!=1:\n",
    "                    if (word.lower() not in stop_words) and (word.lower() not in block_words):\n",
    "                        if vocabulary_dict.get(word.lower())!=None:\n",
    "                            vocabulary_dict[word.lower()]+=1\n",
    "                        else:\n",
    "                            vocabulary_dict[word.lower()] = 1\n",
    "    sorted_vocabulary = sorted(vocabulary_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    vocabulary = []\n",
    "    index = 0\n",
    "    for word,freq in sorted_vocabulary:\n",
    "        vocabulary.append(word)\n",
    "        index+=1\n",
    "        if index==2000:\n",
    "            break\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_dataset(vocabulary,data_dir):\n",
    "    \"\"\"Return a 2D array(input) and a 1D array(ouput)\"\"\"\n",
    "    \n",
    "    x = pd.DataFrame(columns = vocabulary)\n",
    "    y = []\n",
    "    for dir_ in data_dir:\n",
    "        for i in range(len(data_dir[dir_])):\n",
    "            index = len(x)\n",
    "            x.loc[index] = np.zeros(len(vocabulary))\n",
    "            y.append(dir_)\n",
    "            current_path = path + '/' + dir_ + '/' + data_dir[dir_][i]\n",
    "            data_file = open(current_path,'r').read()\n",
    "            for word in data_file.split():\n",
    "                if word.lower() in vocabulary:\n",
    "                    x.loc[index][word.lower()]+=1\n",
    "    x=x.values\n",
    "    return x,np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Fit Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, Y_train):\n",
    "    \"\"\"This function creates dictionary which is trained on the dataset provided\"\"\"\n",
    "    \n",
    "    result = {}\n",
    "    class_values = set(Y_train)\n",
    "    result[\"total_data\"] = len(Y_train)\n",
    "    for current_class in class_values:\n",
    "        result[current_class] = {}\n",
    "        current_class_rows = (Y_train == current_class)\n",
    "        X_train_current = X_train[current_class_rows]\n",
    "        Y_train_current = Y_train[current_class_rows]\n",
    "        num_features = X_train.shape[1]\n",
    "        Sum = 0\n",
    "        for j in range(1, num_features + 1):\n",
    "            result[current_class][j] = (X_train_current[:, j - 1]).sum()\n",
    "            Sum+=result[current_class][j]\n",
    "        result[current_class][\"total_count\"] = Sum\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(dictionary, x, current_class):\n",
    "    \"\"\"This prints the bayes probability of each row with laplace corrections applied. This ouputs\n",
    "    a logarithmic probability\"\"\"\n",
    "    \n",
    "    output = np.log(dictionary[current_class][\"total_count\"]) - np.log(dictionary[\"total_data\"])\n",
    "    num_features = len(dictionary[current_class].keys()) - 1;\n",
    "    for j in range(1, num_features + 1):\n",
    "        count_j_feature = dictionary[current_class][j] + 1\n",
    "        count_current_class = dictionary[current_class][\"total_count\"] + num_features\n",
    "        current_j_probablity = np.log(count_j_feature) - np.log(count_current_class)\n",
    "        for freq in range(int(x[j-1])):\n",
    "            output = output + current_j_probablity\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSinglePoint(dictionary, x):\n",
    "    \"\"\"This predicts the class for each row and stores the class with highest probability\"\"\"\n",
    "    \n",
    "    classes = dictionary.keys()\n",
    "    best_p = -1000\n",
    "    best_class = -1\n",
    "    first_run = True\n",
    "    for current_class in classes:\n",
    "        if (current_class == \"total_data\"):\n",
    "            continue\n",
    "        p_current_class = probability(dictionary, x, current_class)\n",
    "        if (first_run or p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dictionary, X_test):\n",
    "    \"\"\"This function predicts the value for the input dataset and returns an array\"\"\"\n",
    "    \n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        x_class = predictSinglePoint(dictionary, x) # Predict class for each data row\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform datafiles from different folders to store in an array which can be passed to classifier\n",
    "path='./20_newsgroups'\n",
    "train_directory,test_directory=train_test_split_dir(path)\n",
    "dataset_vocabulary=vocabulary(train_directory, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train=structured_dataset(dataset_vocabulary, train_directory)\n",
    "clf1=fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test,y_test=structured_dataset(dataset_vocabulary, test_directory)\n",
    "y_pred=predict(clf1, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[184   2   1   0   0   0   1   3   1   2   1   0   4   5   3   6   2   3\n",
      "    3  29]\n",
      " [  3 193  11   3   6   8   5   1   2   2   1   2   7   3   2   1   0   0\n",
      "    0   0]\n",
      " [  0   8 206   7   0   7   5   1   3   0   3   0   9   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   4  11 193   9   0  17   5   1   0   0   0  10   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   7   6 215   0   4   6   2   1   0   0   2   2   4   0   0   0\n",
      "    0   0]\n",
      " [  0  19  33   4   3 183   2   0   0   2   0   0   4   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   0   9   3   3 213   5   1   2   0   1   5   2   2   0   0   0\n",
      "    0   1]\n",
      " [  0   1   0   0   0   0   6 234   3   1   0   0   2   0   1   0   0   0\n",
      "    2   0]\n",
      " [  0   0   0   0   0   0   3   5 239   0   0   0   2   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0   0   0   1   0   0   4   0   1 238   5   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   2   0   3   5 233   0   0   0   2   1   1   0\n",
      "    2   1]\n",
      " [  0   6   2   0   3   2   1   1   4   1   2 202  13   2   1   0   4   0\n",
      "    6   0]\n",
      " [  1   9   9   5   4   0   3  10   2   2   1   0 200   2   0   0   1   1\n",
      "    0   0]\n",
      " [  8  10   0   1   0   1   3   6   5   1   3   0   6 199   2   0   0   1\n",
      "    1   3]\n",
      " [  5   4   2   0   1   0   2   3   3   3   0   0   1   4 210   0   1   1\n",
      "    6   4]\n",
      " [  0   0   1   0   0   1   0   0   0   0   0   0   0   0   0 248   0   0\n",
      "    0   0]\n",
      " [  1   0   2   0   0   0   7   2   0   1   1   4   0   0   0   0 214   0\n",
      "    7  11]\n",
      " [ 29   1   1   0   2   0   2   1   1   1   0   1   1   5   0   3   3 187\n",
      "    9   3]\n",
      " [  5   1   0   0   0   0   2   1   3   1   0   2   0   3   4   1  55  10\n",
      "  142  20]\n",
      " [ 61   2   0   0   0   0   0   0   1   2   1   2   3   5   1  23  17   1\n",
      "   12 119]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred)) # Prints confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.62      0.74      0.67       250\n",
      "           comp.graphics       0.73      0.77      0.75       250\n",
      " comp.os.ms-windows.misc       0.72      0.82      0.77       250\n",
      "comp.sys.ibm.pc.hardware       0.84      0.77      0.81       250\n",
      "   comp.sys.mac.hardware       0.87      0.86      0.87       250\n",
      "          comp.windows.x       0.89      0.73      0.80       250\n",
      "            misc.forsale       0.76      0.85      0.80       250\n",
      "               rec.autos       0.82      0.94      0.88       250\n",
      "         rec.motorcycles       0.87      0.96      0.91       250\n",
      "      rec.sport.baseball       0.90      0.95      0.92       250\n",
      "        rec.sport.hockey       0.93      0.93      0.93       250\n",
      "               sci.crypt       0.94      0.81      0.87       250\n",
      "         sci.electronics       0.74      0.80      0.77       250\n",
      "                 sci.med       0.86      0.80      0.83       250\n",
      "               sci.space       0.90      0.84      0.87       250\n",
      "  soc.religion.christian       0.88      0.99      0.93       250\n",
      "      talk.politics.guns       0.72      0.86      0.78       250\n",
      "   talk.politics.mideast       0.92      0.75      0.82       250\n",
      "      talk.politics.misc       0.74      0.57      0.64       250\n",
      "      talk.religion.misc       0.62      0.48      0.54       250\n",
      "\n",
      "             avg / total       0.81      0.81      0.81      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred)) # Prints classification report for testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cross Check With Sklearn Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = MultinomialNB()\n",
    "clf2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[184   2   1   0   0   0   1   3   2   2   1   0   4   5   3   6   1   2\n",
      "    3  30]\n",
      " [  3 191  11   3   6   8   5   1   2   2   2   1   8   4   2   1   0   0\n",
      "    0   0]\n",
      " [  0   8 207   7   0   7   8   1   2   0   1   0   8   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   4  10 192   9   0  19   4   2   0   0   0  10   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   7   6 213   0   6   6   2   1   0   0   3   1   4   0   0   0\n",
      "    0   0]\n",
      " [  0  19  32   4   3 182   4   0   0   2   0   0   4   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0   9   3   3 218   5   1   1   0   1   2   2   2   0   0   0\n",
      "    0   1]\n",
      " [  0   1   0   0   0   0   8 231   4   1   0   0   2   0   1   0   0   0\n",
      "    2   0]\n",
      " [  0   0   0   0   0   0   3   5 239   0   0   0   2   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0   0   0   1   0   0   4   0   1 238   5   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   2   0   3   5 233   0   0   0   2   1   1   0\n",
      "    2   1]\n",
      " [  0   7   2   0   4   2   3   1   5   0   1 199  14   2   1   0   4   0\n",
      "    5   0]\n",
      " [  1   9   9   5   4   0   4  10   2   2   1   0 199   2   0   0   1   1\n",
      "    0   0]\n",
      " [  8   9   0   0   0   1   6   6   7   1   3   0   5 198   1   0   0   1\n",
      "    1   3]\n",
      " [  5   3   3   0   1   0   2   3   3   3   0   0   1   4 210   0   1   1\n",
      "    6   4]\n",
      " [  0   0   1   0   0   1   0   0   0   0   0   0   0   0   0 248   0   0\n",
      "    0   0]\n",
      " [  0   0   2   0   0   0   7   2   2   1   1   4   1   0   0   0 212   0\n",
      "    6  12]\n",
      " [ 33   1   1   0   2   0   2   1   1   1   0   1   1   5   0   3   3 183\n",
      "    9   3]\n",
      " [  5   1   0   0   0   0   3   1   4   1   0   2   0   3   4   1  57  10\n",
      "  138  20]\n",
      " [ 60   3   0   0   0   0   0   0   3   2   0   1   4   5   2  23  18   1\n",
      "    9 119]]\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = clf2.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred2)) # Prints confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.62      0.74      0.67       250\n",
      "           comp.graphics       0.73      0.76      0.75       250\n",
      " comp.os.ms-windows.misc       0.72      0.83      0.77       250\n",
      "comp.sys.ibm.pc.hardware       0.85      0.77      0.81       250\n",
      "   comp.sys.mac.hardware       0.87      0.85      0.86       250\n",
      "          comp.windows.x       0.89      0.73      0.80       250\n",
      "            misc.forsale       0.71      0.87      0.79       250\n",
      "               rec.autos       0.82      0.92      0.87       250\n",
      "         rec.motorcycles       0.84      0.96      0.89       250\n",
      "      rec.sport.baseball       0.90      0.95      0.93       250\n",
      "        rec.sport.hockey       0.94      0.93      0.94       250\n",
      "               sci.crypt       0.95      0.80      0.87       250\n",
      "         sci.electronics       0.74      0.80      0.77       250\n",
      "                 sci.med       0.86      0.79      0.82       250\n",
      "               sci.space       0.90      0.84      0.87       250\n",
      "  soc.religion.christian       0.88      0.99      0.93       250\n",
      "      talk.politics.guns       0.71      0.85      0.77       250\n",
      "   talk.politics.mideast       0.92      0.73      0.82       250\n",
      "      talk.politics.misc       0.76      0.55      0.64       250\n",
      "      talk.religion.misc       0.62      0.48      0.54       250\n",
      "\n",
      "             avg / total       0.81      0.81      0.80      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred2)) # Prints classification report for testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Accuracy Comparison Between Self Implemented And Sklearn Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self Implemented Naive Bayes Accuracy:  0.8104\n",
      "Sklearn Naive Bayes Accuracy:  0.8068\n"
     ]
    }
   ],
   "source": [
    "print(\"Self Implemented Naive Bayes Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Sklearn Naive Bayes Accuracy: \", accuracy_score(y_test, y_pred2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
